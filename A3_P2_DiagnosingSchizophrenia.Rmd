---
title: "Assignment 3 - Part 2 - Diagnosing Schizophrenia from Voice"
author: "Riccardo Fusaroli"
date: "October 17, 2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

## Assignment 3 - Diagnosing schizophrenia from voice

In the previous part of the assignment you generated a bunch of "features", that is, of quantitative descriptors of voice in schizophrenia, focusing on pitch.
In the course of this assignment we will use them to try to automatically diagnose schizophrenia from voice only, that is, relying on the set of features you produced last time, we will try to produce an automated classifier.

```{r, include = FALSE}
setwd("~/Cognitive Science/3. Semester/Experimental Methods 3/Portfolio-2/")
library(pacman)
p_load(tidyverse, stringr, Metrics, caret, lme4, simr, lmerTest, stats, FinCal, PerformanceAnalytics, nonlinearTseries, GMCM, pROC, createFolds)

pitch_data <- read.csv(file = "pitch_data.csv")

#NA_pitch_data <- na.omit(pitch_data, col="Study")
```


### Question 1: Can you diagnose schizophrenia from pitch range only? If so, how well?

Build a logistic regression to see whether you can diagnose schizophrenia from pitch range only.

```{r, include = FALSE}
#Plotting a single data point
pitch_data1=pitch_data[pitch_data$trial==1,]
pitch_data1=filter(pitch_data1, pitch_data1$Sub_Diag==1010)
ggplot(pitch_data1, aes(range,diagnosis,colour=diagnosis)) + geom_point() + theme_classic()


#Making logistic regression
m1 <- glm(diagnosis ~ range, pitch_data, family="binomial")
summary(m1)

```

Calculate the different performance measures (accuracy, sensitivity, specificity, PPV, NPV, ROC curve) on a logistic regression using the full dataset. Don't forget the random effects!
```{r, include = FALSE}
#rescaling
for (i in 10:18){ 
  pitch_data[,i] = scales::rescale(pitch_data[,i], to=c(0,1))
}

#causing a problem: pitch_data$scaled_range <- scale(pitch_data$range, center = TRUE, scale = TRUE) 

m2 <- glmer(diagnosis ~ range+(1+trial|Subject)+(1|study), pitch_data, family="binomial")
summary(m2)

#Accuracy
pitch_data$PredictionsPerc=GMCM:::inv.logit(predict(m2))
pitch_data$Predictions[pitch_data$PredictionsPerc>0.5]="1"
pitch_data$Predictions[pitch_data$PredictionsPerc<=0.5]="0"
confusionMatrix(data = pitch_data$Predictions, reference = pitch_data$diagnosis, positive = "1") 

Accuracy = (326+490)/(326+490+338+185) 

Sensitivity = 490/(185+490) 
Specificity = 326/(326+338)

pitch_data$diagnosis <- as.factor(pitch_data$diagnosis)
pitch_data$Predictions <- as.factor(pitch_data$Predictions)

posPredValue(data = pitch_data$Predictions, reference = pitch_data$diagnosis, positive = "1") 
negPredValue(data = pitch_data$Predictions, reference = pitch_data$diagnosis, negative = "0")

rock <- roc(response = pitch_data$diagnosis, predictor = pitch_data$PredictionsPerc)
auc(rock) 
ci (rock)
plot(rock, legacy.axes = TRUE) 

```


Then cross-validate the logistic regression and re-calculate performance on the testing folds. N.B. The cross-validation functions you already have should be tweaked: you need to calculate these new performance measures.

```{r, include = FALSE}
set.seed(3)
pitch_data$diagnosis<- as.factor(pitch_data$diagnosis)
folds <- createFolds(unique(pitch_data$Subject), 10)
pitch_data$Subject <- as.numeric(as.factor(pitch_data$Subject))
RangePred = rep(NA, nrow(pitch_data))


# Loop for range feature
for (i in 1:length(folds)){
  f <- folds[[i]]
  train = filter(pitch_data,!(Subject %in% f))
  test = filter(pitch_data,(Subject %in% f))
  model = glmer(diagnosis ~ range + (1+trial|Subject) + (1|study), train, family="binomial")
  pitch_data$range_pred[pitch_data$Subject %in% f] = GMCM:::inv.logit(predict(model, test, allow.new.levels = TRUE))
  pitch_data$range_diag[pitch_data$range_pred>0.5]="1"
  pitch_data$range_diag[pitch_data$range_pred<=0.5]="0"
  pitch_data$range_diag <- as.factor(pitch_data$range_diag)
  accuracyTest <- accuracy(pitch_data$range_diag[which(pitch_data$Subject %in% f)],  pitch_data$diagnosis[which(pitch_data$Subject %in% f)])
  sensitivityTest <- sensitivity(pitch_data$range_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1")
  specificityTest <- specificity(pitch_data$range_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0") 
  ppvTest <- posPredValue(pitch_data$range_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1") 
  npvTest <- negPredValue(pitch_data$range_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0")
  temp_df <- data_frame(accuracyTest = accuracyTest, sensitivityTest = sensitivityTest, specificityTest = specificityTest, ppvTest = ppvTest, npvTest = npvTest, fold_nr = i)
  if (i == 1){
    result_df <- temp_df
  } else {
    result_df <- rbind(result_df, temp_df)
  }
}

range_performance_means <- colMeans(result_df[-6])
range_performance_means

```

N.B. the predict() function generates log odds (the full scale between minus and plus infinity). Log odds > 0 indicates a choice of 1, below a choice of 0.
N.B. you need to decide whether calculate performance on each single test fold or save all the prediction for test folds in one datase, so to calculate overall performance.
N.B. Now you have two levels of structure: subject and study. Should this impact your cross-validation?

RESPONSE: Yes, we can significantly predict diagnosis based on range (p-value = 0.007). 

   accuracyTest sensitivityTest specificityTest         ppvTest         npvTest 
      0.5592175       0.7204767       0.3979498       0.5493280       0.5949901

As seen in the table, the sensitivity of the model is relatively large (sensitivity = 0.72), this means that we capture a rather large amount of the schizophrenic with the model. However in turn, our specificity is relatively small. Thus, we have a relatively large chance of making false positives i.e. diagnosing a participant with schizophrenia, who is a control.

### Question 2 - Which single acoustic predictor is the best predictor of diagnosis?
```{r, include = FALSE}
set.seed(3)
pitch_data$diagnosis<- as.factor(pitch_data$diagnosis)
folds <- createFolds(unique(pitch_data$Subject), 10)
pitch_data$Subject <- as.numeric(as.factor(pitch_data$Subject))
meanPred = rep(NA, nrow(pitch_data))


# Loop for mean feature
for (i in 1:length(folds)){
  f <- folds[[i]]
  train = filter(pitch_data,!(Subject %in% f))
  test = filter(pitch_data,(Subject %in% f))
  model = glmer(diagnosis ~ mean + (1+trial|Subject) + (1|study), train, family="binomial")
  pitch_data$mean_pred[pitch_data$Subject %in% f] = GMCM:::inv.logit(predict(model, test, allow.new.levels = TRUE))
  pitch_data$mean_diag[pitch_data$mean_pred>0.5]="1"
  pitch_data$mean_diag[pitch_data$mean_pred<=0.5]="0"
  pitch_data$mean_diag <- as.factor(pitch_data$mean_diag)
  accuracyTest <- accuracy(pitch_data$mean_diag[which(pitch_data$Subject %in% f)],  pitch_data$diagnosis[which(pitch_data$Subject %in% f)])
  sensitivityTest <- sensitivity(pitch_data$mean_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1")
  specificityTest <- specificity(pitch_data$mean_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0") 
  ppvTest <- posPredValue(pitch_data$mean_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1") 
  npvTest <- negPredValue(pitch_data$mean_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0")
  temp_df <- data_frame(accuracyTest = accuracyTest, sensitivityTest = sensitivityTest, specificityTest = specificityTest, ppvTest = ppvTest, npvTest = npvTest, fold_nr = i)
  if (i == 1){
    result_df <- temp_df
  } else {
    result_df <- rbind(result_df, temp_df)
  }
}

mean_performance_means <- colMeans(result_df[-6])
mean_performance_means

#min
set.seed(3)
pitch_data$diagnosis<- as.factor(pitch_data$diagnosis)
folds <- createFolds(unique(pitch_data$Subject), 10)
pitch_data$Subject <- as.numeric(as.factor(pitch_data$Subject))
minPred = rep(NA, nrow(pitch_data))


# Loop for min feature
for (i in 1:length(folds)){
  f <- folds[[i]]
  train = filter(pitch_data,!(Subject %in% f))
  test = filter(pitch_data,(Subject %in% f))
  model = glmer(diagnosis ~ min + (1+trial|Subject) + (1|study), train, family="binomial")
  pitch_data$min_pred[pitch_data$Subject %in% f] = GMCM:::inv.logit(predict(model, test, allow.new.levels = TRUE))
  pitch_data$min_diag[pitch_data$min_pred>0.5]="1"
  pitch_data$min_diag[pitch_data$min_pred<=0.5]="0"
  pitch_data$min_diag <- as.factor(pitch_data$min_diag)
  accuracyTest <- accuracy(pitch_data$min_diag[which(pitch_data$Subject %in% f)],  pitch_data$diagnosis[which(pitch_data$Subject %in% f)])
  sensitivityTest <- sensitivity(pitch_data$min_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1")
  specificityTest <- specificity(pitch_data$min_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0") 
  ppvTest <- posPredValue(pitch_data$min_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1") 
  npvTest <- negPredValue(pitch_data$min_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0")
  temp_df <- data_frame(accuracyTest = accuracyTest, sensitivityTest = sensitivityTest, specificityTest = specificityTest, ppvTest = ppvTest, npvTest = npvTest, fold_nr = i)
  if (i == 1){
    result_df <- temp_df
  } else {
    result_df <- rbind(result_df, temp_df)
  }
}

min_performance_means <- colMeans(result_df[-6])
min_performance_means
#failed to converge


#max
set.seed(3)
pitch_data$diagnosis<- as.factor(pitch_data$diagnosis)
folds <- createFolds(unique(pitch_data$Subject), 10)
pitch_data$Subject <- as.numeric(as.factor(pitch_data$Subject))
maxPred = rep(NA, nrow(pitch_data))


# Loop for max feature
for (i in 1:length(folds)){
  f <- folds[[i]]
  train = filter(pitch_data,!(Subject %in% f))
  test = filter(pitch_data,(Subject %in% f))
  model = glmer(diagnosis ~ max + (1+trial|Subject) + (1|study), train, family="binomial")
  pitch_data$max_pred[pitch_data$Subject %in% f] = GMCM:::inv.logit(predict(model, test, allow.new.levels = TRUE))
  pitch_data$max_diag[pitch_data$max_pred>0.5]="1"
  pitch_data$max_diag[pitch_data$max_pred<=0.5]="0"
  pitch_data$max_diag <- as.factor(pitch_data$max_diag)
  accuracyTest <- accuracy(pitch_data$max_diag[which(pitch_data$Subject %in% f)],  pitch_data$diagnosis[which(pitch_data$Subject %in% f)])
  sensitivityTest <- sensitivity(pitch_data$max_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1")
  specificityTest <- specificity(pitch_data$max_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0") 
  ppvTest <- posPredValue(pitch_data$max_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1") 
  npvTest <- negPredValue(pitch_data$max_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0")
  temp_df <- data_frame(accuracyTest = accuracyTest, sensitivityTest = sensitivityTest, specificityTest = specificityTest, ppvTest = ppvTest, npvTest = npvTest, fold_nr = i)
  if (i == 1){
    result_df <- temp_df
  } else {
    result_df <- rbind(result_df, temp_df)
  }
}

max_performance_means <- colMeans(result_df[-6])
max_performance_means

#sd
set.seed(3)
pitch_data$diagnosis<- as.factor(pitch_data$diagnosis)
folds <- createFolds(unique(pitch_data$Subject), 10)
pitch_data$Subject <- as.numeric(as.factor(pitch_data$Subject))
sdPred = rep(NA, nrow(pitch_data))


# Loop for sd feature
for (i in 1:length(folds)){
  f <- folds[[i]]
  train = filter(pitch_data,!(Subject %in% f))
  test = filter(pitch_data,(Subject %in% f))
  model = glmer(diagnosis ~ sd + (1+trial|Subject) + (1|study), train, family="binomial")
  pitch_data$sd_pred[pitch_data$Subject %in% f] = GMCM:::inv.logit(predict(model, test, allow.new.levels = TRUE))
  pitch_data$sd_diag[pitch_data$sd_pred>0.5]="1"
  pitch_data$sd_diag[pitch_data$sd_pred<=0.5]="0"
  pitch_data$sd_diag <- as.factor(pitch_data$sd_diag)
  accuracyTest <- accuracy(pitch_data$sd_diag[which(pitch_data$Subject %in% f)],  pitch_data$diagnosis[which(pitch_data$Subject %in% f)])
  sensitivityTest <- sensitivity(pitch_data$sd_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1")
  specificityTest <- specificity(pitch_data$sd_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0") 
  ppvTest <- posPredValue(pitch_data$sd_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1") 
  npvTest <- negPredValue(pitch_data$sd_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0")
  temp_df <- data_frame(accuracyTest = accuracyTest, sensitivityTest = sensitivityTest, specificityTest = specificityTest, ppvTest = ppvTest, npvTest = npvTest, fold_nr = i)
  if (i == 1){
    result_df <- temp_df
  } else {
    result_df <- rbind(result_df, temp_df)
  }
}

sd_performance_means <- colMeans(result_df[-6])
sd_performance_means

#median
set.seed(3)
pitch_data$diagnosis<- as.factor(pitch_data$diagnosis)
folds <- createFolds(unique(pitch_data$Subject), 10)
pitch_data$Subject <- as.numeric(as.factor(pitch_data$Subject))
medianPred = rep(NA, nrow(pitch_data))


# Loop for median feature
for (i in 1:length(folds)){
  f <- folds[[i]]
  train = filter(pitch_data,!(Subject %in% f))
  test = filter(pitch_data,(Subject %in% f))
  model = glmer(diagnosis ~ median + (1+trial|Subject) + (1|study), train, family="binomial")
  pitch_data$median_pred[pitch_data$Subject %in% f] = GMCM:::inv.logit(predict(model, test, allow.new.levels = TRUE))
  pitch_data$median_diag[pitch_data$median_pred>0.5]="1"
  pitch_data$median_diag[pitch_data$median_pred<=0.5]="0"
  pitch_data$median_diag <- as.factor(pitch_data$median_diag)
  accuracyTest <- accuracy(pitch_data$median_diag[which(pitch_data$Subject %in% f)],  pitch_data$diagnosis[which(pitch_data$Subject %in% f)])
  sensitivityTest <- sensitivity(pitch_data$median_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1")
  specificityTest <- specificity(pitch_data$median_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0") 
  ppvTest <- posPredValue(pitch_data$median_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1") 
  npvTest <- negPredValue(pitch_data$median_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0")
  temp_df <- data_frame(accuracyTest = accuracyTest, sensitivityTest = sensitivityTest, specificityTest = specificityTest, ppvTest = ppvTest, npvTest = npvTest, fold_nr = i)
  if (i == 1){
    result_df <- temp_df
  } else {
    result_df <- rbind(result_df, temp_df)
  }
}

median_performance_means <- colMeans(result_df[-6])
median_performance_means


#iqr
set.seed(3)
pitch_data$diagnosis<- as.factor(pitch_data$diagnosis)
folds <- createFolds(unique(pitch_data$Subject), 10)
pitch_data$Subject <- as.numeric(as.factor(pitch_data$Subject))
iqrPred = rep(NA, nrow(pitch_data))


# Loop for iqr feature
for (i in 1:length(folds)){
  f <- folds[[i]]
  train = filter(pitch_data,!(Subject %in% f))
  test = filter(pitch_data,(Subject %in% f))
  model = glmer(diagnosis ~ iqr + (1+trial|Subject) + (1|study), train, family="binomial")
  pitch_data$iqr_pred[pitch_data$Subject %in% f] = GMCM:::inv.logit(predict(model, test, allow.new.levels = TRUE))
  pitch_data$iqr_diag[pitch_data$iqr_pred>0.5]="1"
  pitch_data$iqr_diag[pitch_data$iqr_pred<=0.5]="0"
  pitch_data$iqr_diag <- as.factor(pitch_data$iqr_diag)
  accuracyTest <- accuracy(pitch_data$iqr_diag[which(pitch_data$Subject %in% f)],  pitch_data$diagnosis[which(pitch_data$Subject %in% f)])
  sensitivityTest <- sensitivity(pitch_data$iqr_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1")
  specificityTest <- specificity(pitch_data$iqr_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0") 
  ppvTest <- posPredValue(pitch_data$iqr_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1") 
  npvTest <- negPredValue(pitch_data$iqr_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0")
  temp_df <- data_frame(accuracyTest = accuracyTest, sensitivityTest = sensitivityTest, specificityTest = specificityTest, ppvTest = ppvTest, npvTest = npvTest, fold_nr = i)
  if (i == 1){
    result_df <- temp_df
  } else {
    result_df <- rbind(result_df, temp_df)
  }
}

iqr_performance_means <- colMeans(result_df[-6])
iqr_performance_means

#mad
set.seed(3)
pitch_data$diagnosis<- as.factor(pitch_data$diagnosis)
folds <- createFolds(unique(pitch_data$Subject), 10)
pitch_data$Subject <- as.numeric(as.factor(pitch_data$Subject))
madPred = rep(NA, nrow(pitch_data))


# Loop for mad feature
for (i in 1:length(folds)){
  f <- folds[[i]]
  train = filter(pitch_data,!(Subject %in% f))
  test = filter(pitch_data,(Subject %in% f))
  model = glmer(diagnosis ~ mad + (1+trial|Subject) + (1|study), train, family="binomial")
  pitch_data$mad_pred[pitch_data$Subject %in% f] = GMCM:::inv.logit(predict(model, test, allow.new.levels = TRUE))
  pitch_data$mad_diag[pitch_data$mad_pred>0.5]="1"
  pitch_data$mad_diag[pitch_data$mad_pred<=0.5]="0"
  pitch_data$mad_diag <- as.factor(pitch_data$mad_diag)
  accuracyTest <- accuracy(pitch_data$mad_diag[which(pitch_data$Subject %in% f)],  pitch_data$diagnosis[which(pitch_data$Subject %in% f)])
  sensitivityTest <- sensitivity(pitch_data$mad_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1")
  specificityTest <- specificity(pitch_data$mad_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0") 
  ppvTest <- posPredValue(pitch_data$mad_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1") 
  npvTest <- negPredValue(pitch_data$mad_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0")
  temp_df <- data_frame(accuracyTest = accuracyTest, sensitivityTest = sensitivityTest, specificityTest = specificityTest, ppvTest = ppvTest, npvTest = npvTest, fold_nr = i)
  if (i == 1){
    result_df <- temp_df
  } else {
    result_df <- rbind(result_df, temp_df)
  }
}

mad_performance_means <- colMeans(result_df[-6])
mad_performance_means


#coefvar
set.seed(3)
pitch_data$diagnosis<- as.factor(pitch_data$diagnosis)
folds <- createFolds(unique(pitch_data$Subject), 10)
pitch_data$Subject <- as.numeric(as.factor(pitch_data$Subject))
coefvarPred = rep(NA, nrow(pitch_data))


# Loop for coefvar feature
for (i in 1:length(folds)){
  f <- folds[[i]]
  train = filter(pitch_data,!(Subject %in% f))
  test = filter(pitch_data,(Subject %in% f))
  model = glmer(diagnosis ~ coefvar + (1+trial|Subject) + (1|study), train, family="binomial")
  pitch_data$coefvar_pred[pitch_data$Subject %in% f] = GMCM:::inv.logit(predict(model, test, allow.new.levels = TRUE))
  pitch_data$coefvar_diag[pitch_data$coefvar_pred>0.5]="1"
  pitch_data$coefvar_diag[pitch_data$coefvar_pred<=0.5]="0"
  pitch_data$coefvar_diag <- as.factor(pitch_data$coefvar_diag)
  accuracyTest <- accuracy(pitch_data$coefvar_diag[which(pitch_data$Subject %in% f)],  pitch_data$diagnosis[which(pitch_data$Subject %in% f)])
  sensitivityTest <- sensitivity(pitch_data$coefvar_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1")
  specificityTest <- specificity(pitch_data$coefvar_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0") 
  ppvTest <- posPredValue(pitch_data$coefvar_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1") 
  npvTest <- negPredValue(pitch_data$coefvar_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0")
  temp_df <- data_frame(accuracyTest = accuracyTest, sensitivityTest = sensitivityTest, specificityTest = specificityTest, ppvTest = ppvTest, npvTest = npvTest, fold_nr = i)
  if (i == 1){
    result_df <- temp_df
  } else {
    result_df <- rbind(result_df, temp_df)
  }
}

coefvar_performance_means <- colMeans(result_df[-6])
coefvar_performance_means
```


```{r, include = FALSE}

compared <- rbind(coefvar_performance_means, mean_performance_means, mad_performance_means, iqr_performance_means, median_performance_means, sd_performance_means, max_performance_means, min_performance_means, range_performance_means)

compared

```
RESULT: 
From the point of view of diagnosing as many schizopherinic as possible so no schizophrenic goes without treatment, we aim for that our model has a relatively high sensitivity and accuracy. Based on this, the best simple feature model, we can produce, is diagnosis explained by interquartile range (IQR) and as random effects a random intercept for the effect of study together with a by-subject random slope for trial (diagnosis ~ iqr + (1+trial|Subject) + (1|study)) 

### Question 3 - Which combination of acoustic predictors is best for diagnosing schizophrenia?

Now it's time to go wild! Use all (voice-related) variables and interactions you can think of. Compare models and select the best performing model you can find.

Remember:
- Out-of-sample error crucial to build the best model!
- After choosing the model, send Malte and Riccardo the code of your model

```{r, include = FALSE}
set.seed(3)
pitch_data$diagnosis<- as.factor(pitch_data$diagnosis)
folds <- createFolds(unique(pitch_data$Subject), 10)
pitch_data$Subject <- as.numeric(as.factor(pitch_data$Subject))
meanPred = rep(NA, nrow(pitch_data))


# Loop for mean feature
for (i in 1:length(folds)){
  f <- folds[[i]]
  train = filter(pitch_data,!(Subject %in% f))
  test = filter(pitch_data,(Subject %in% f))
  model = glmer(diagnosis ~ mean + (1+trial|Subject) + (1|study), train, family="binomial")
  pitch_data$mean_pred[pitch_data$Subject %in% f] = GMCM:::inv.logit(predict(model, test, allow.new.levels = TRUE))
  pitch_data$mean_diag[pitch_data$mean_pred>0.5]="1"
  pitch_data$mean_diag[pitch_data$mean_pred<=0.5]="0"
  pitch_data$mean_diag <- as.factor(pitch_data$mean_diag)
  accuracyTest <- accuracy(pitch_data$mean_diag[which(pitch_data$Subject %in% f)],  pitch_data$diagnosis[which(pitch_data$Subject %in% f)])
  sensitivityTest <- sensitivity(pitch_data$mean_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1")
  specificityTest <- specificity(pitch_data$mean_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0") 
  ppvTest <- posPredValue(pitch_data$mean_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], positive = "1") 
  npvTest <- negPredValue(pitch_data$mean_diag[which(pitch_data$Subject %in% f)], reference = pitch_data$diagnosis[which(pitch_data$Subject %in% f)], negative = "0")
  temp_df <- data_frame(accuracyTest = accuracyTest, sensitivityTest = sensitivityTest, specificityTest = specificityTest, ppvTest = ppvTest, npvTest = npvTest, fold_nr = i)
  if (i == 1){
    result_df <- temp_df
  } else {
    result_df <- rbind(result_df, temp_df)
  }
}

mean_performance_means <- colMeans(result_df[-6])
mean_performance_means

#Previous best model 
model = glmer(diagnosis ~ iqr + (1+trial|Subject) + (1|study), train, family="binomial")

#Models that did not fail to converge in cv: 
model = glmer(diagnosis ~ range + iqr + (1+trial|Subject) + (1|study), train, family="binomial") # A: 0.56, SEN: 0.73
model = glmer(diagnosis ~ iqr + sd + (1+trial|Subject) + (1|study), train, family="binomial") # A: 0.58, SEN: 0.84
model = glmer(diagnosis ~ iqr + mad + (1+trial|Subject) + (1|study), train, family="binomial") # A: 0.56, SEN: 0.74
model = glmer(diagnosis ~ iqr + max + (1+trial|Subject) + (1|study), train, family="binomial") # A: 0.52, SEN: 0.63

#Models that fail to converge in cv:
#- with interaction:
model = glmer(diagnosis ~ range*iqr + (1+trial|Subject) + (1|study), train, family="binomial")
model = glmer(diagnosis ~ iqr*mean + range + (1+trial|Subject) + (1|study), train, family="binomial")
model = glmer(diagnosis ~ range*iqr*mean + (1+trial|Subject) + (1|study), train, family="binomial")
model = glmer(diagnosis ~ iqr*sd + (1+trial|Subject) + (1|study), train, family="binomial")
model = glmer(diagnosis ~ iqr*mad + range + (1+trial|Subject) + (1|study), train, family="binomial")
model = glmer(diagnosis ~ iqr*coefvar + (1+trial|Subject) + (1|study), train, family="binomial")
model = glmer(diagnosis ~ iqr*max + (1+trial|Subject) + (1|study), train, family="binomial")

#- without interaction:
model = glmer(diagnosis ~ range + iqr + mean + (1+trial|Subject) + (1|study), train, family="binomial")
model = glmer(diagnosis ~ range + iqr + sd + (1+trial|Subject) + (1|study), train, family="binomial")
model = glmer(diagnosis ~ range + iqr + mad + (1+trial|Subject) + (1|study), train, family="binomial")
model = glmer(diagnosis ~ range + sd + (1+trial|Subject) + (1|study), train, family="binomial")
model = glmer(diagnosis ~ sd + iqr + mad + (1+trial|Subject) + (1|study), train, family="binomial")
model = glmer(diagnosis ~ sd + iqr + coefvar + (1+trial|Subject) + (1|study), train, family="binomial")
model = glmer(diagnosis ~ iqr + coefvar + (1+trial|Subject) + (1|study), train, family="binomial")
model = glmer(diagnosis ~ iqr + sd + mean + (1+trial|Subject) + (1|study), train, family="binomial")
model = glmer(diagnosis ~ iqr + min + (1+trial|Subject) + (1|study), train, family="binomial")
model = glmer(diagnosis ~ coefvar + sd + (1+trial|Subject) + (1|study), train, family="binomial")

model = glmer(diagnosis ~ iqr + (1+trial|Subject) + (1|study), train, family="binomial")
summary(model)
```

RESULT: Using crossvalidation to test the performance of the different constructed models, we find that no other converging model tops the IQR-model from question 2. Thus, the best performing model is still diagnosis explained by interquartile range (IQR) and as random effects a random intercept for the effect of study together with a by-subject random slope for trial (diagnosis ~ iqr + (1+trial|Subject) + (1|study)) 

### Question 4: Properly report the results

METHODS SECTION: how did you analyse the data? That is, how did you extract the data, designed the models and compared their performance?

RESULTS SECTION: can you diagnose schizophrenia based on voice? which features are used? Comment on the difference between the different performance measures.

METHODS: The pitch data from the recordings of schizophrenic and control participants was quite comprehensive. In order to make this more manageable, we downsampled by extracting acoustic features  of the recordings per participant per trial i.e. range, mean, max, min, iqr, etc.  

To test diagnosis predicted by the acoustic features, we constructed a 10-fold crossvalidation, comparing models predicting diagnosis by acoustic features with similar random effects. Crossvalidation allows us to minimize the out-sample-error, so our model will deal better with new data.   

To rate the performance of the model, we extracted measures such as accuracy, sensitivity, specificity, positive prediction values and negative prediction values. From the point of view of diagnosing as many schizopherinic as possible so no schizophrenic goes without treatment, we aim for that our model has a relatively high sensitivity and accuracy. 

RESULTS: The model with the highest sensitivity and accuracy measure is the model, where diagnosis is explained by the interquartile range (IQR) and as random effects a random intercept for the effect of study together with a by-subject random slope for trial (diagnosis ~ iqr + (1+trial|Subject) + (1|study)). The model does significantly predict diagnosis based on the effect of IQR (p-value = 0.043), however, based on the accuracy measure, we are not much better than chance diagnosing schizophrenia (accuracy = 0.59). 

### Bonus question 5

You have some additional bonus data involving speech rate, pauses, etc. Include them in your analysis. Do they improve classification?

### Bonus question 6
Logistic regression is only one of many classification algorithms. Try using others and compare performance. Some examples: Discriminant Function, Random Forest, Support Vector Machine, etc. The package caret provides them.

```{r, include = FALSE}
spell_check_package()
```

